{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOtXSk02w13pwms4//vl28c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bachi-mohamed-rafik/ML_Project/blob/main/Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9TPVMMVzOnS"
      },
      "source": [
        "# Text Preprocessing in Python: Steps, Tools, and Examples\r\n",
        "https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV05Hibey0IQ",
        "outputId": "83c86188-5748-4bdf-e731-ea7f799d4d9f"
      },
      "source": [
        "#Convert text to lower case\r\n",
        "\r\n",
        "str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\r\n",
        "print(str.lower())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8w3pSjr3mBR",
        "outputId": "2ea44481-0524-4463-f0c3-53d318b47cf0"
      },
      "source": [
        "#Numbers Removing\r\n",
        "import re\r\n",
        "input_str = \"Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.\"\r\n",
        "result = re.sub(r'\\d+', '', input_str)\r\n",
        "print(result)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "435RgonF4v90",
        "outputId": "a46c71c5-3f28-4321-fd8b-49c79a0575af"
      },
      "source": [
        "#Remove ponctuation\r\n",
        "'''\r\n",
        "import bytarray\r\n",
        "input_str = \"This &is [an] example? {of} string. with.? punctuation!!!!\" # Sample string\r\n",
        "result = input_str.translate(bytes.maketrans(\"\",'' ''), bytes.punctuation)\r\n",
        "print(result)\r\n",
        "\r\n",
        "\r\n",
        "print(dir(bytes))\r\n",
        "'''"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport bytarray\\ninput_str = \"This &is [an] example? {of} string. with.? punctuation!!!!\" # Sample string\\nresult = input_str.translate(bytes.maketrans(\"\",\\'\\' \\'\\'), bytes.punctuation)\\nprint(result)\\n\\n\\nprint(dir(bytes))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvsTDWg0NlcT",
        "outputId": "e55cfd90-d6d1-45c6-fc14-b71145ccec8a"
      },
      "source": [
        "#Remove str\r\n",
        "str =\" \\t a string example\\t \"\r\n",
        "print(str.strip())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a string example\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga6BKy4Zbhku",
        "outputId": "6430f5b7-2e2e-4d84-84fa-8e19bc27bf23"
      },
      "source": [
        "#Remove stop words\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\r\n",
        "stop_words = set(stopwords.words('english'))\r\n",
        "print(\"stop_words:\\n\",stop_words)\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "tokens=word_tokenize(input_str)\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "result=[i for i in tokens if not i in stop_words]\r\n",
        "print(\"result :\\n\",result)\r\n",
        "\r\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "#ENGLISH_STOP_WORDS"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "stop_words:\n",
            " {'above', 'no', 'same', 'why', 'be', 'between', 're', 'needn', 'few', 'me', 'doesn', 'them', 'this', 'so', \"mustn't\", 'that', 'after', 'before', 'has', \"should've\", 'to', 'shan', 'here', 'into', 'while', 'some', 'her', 'once', 'weren', 'below', 'll', \"shouldn't\", 'what', \"you'll\", 'who', 'm', 'on', 'if', 'now', 'for', 'ourselves', 'hasn', 've', 'any', 'not', 'she', 'down', 'was', \"won't\", 'yourself', 'during', 'ain', 'hers', 'how', 'as', 'we', 'when', 'than', 'wouldn', 'against', 'then', 'each', \"hadn't\", 'through', 'being', \"haven't\", \"needn't\", \"shan't\", \"aren't\", 'they', 'my', 'doing', 'him', 'isn', 'which', 'their', 'with', \"you've\", 'theirs', 'am', \"don't\", 'under', 'myself', 'a', 'himself', 'itself', 'of', 'haven', 'there', 'by', 'where', 'having', 'o', 'in', 'does', 'off', 'but', 'our', 'just', 'your', \"mightn't\", 'up', 'only', 'ma', \"that'll\", 'herself', 'most', 'don', 's', 'its', 'did', 'd', 'an', 'were', \"didn't\", 'it', \"you'd\", 'aren', 'have', \"wouldn't\", 'at', 'nor', 'hadn', \"hasn't\", 'whom', 'shouldn', 'other', 'is', 'didn', 'further', 'y', \"it's\", 'themselves', 'couldn', 'such', 'i', 'ours', 'again', 't', 'about', \"you're\", 'until', \"doesn't\", 'from', 'been', \"wasn't\", 'very', 'won', 'all', 'can', 'over', 'will', 'the', 'had', 'he', \"weren't\", 'out', 'more', 'own', 'or', 'his', \"couldn't\", \"she's\", 'both', 'too', 'should', 'you', 'mightn', 'yours', 'mustn', 'do', 'yourselves', 'those', 'these', 'are', 'and', 'because', 'wasn', \"isn't\"}\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n",
            "result :\n",
            " ['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I33xd4UzoTAi",
        "outputId": "1e148683-6410-41aa-9e08-3a3ba40f37e4"
      },
      "source": [
        "#Stemming using NLTK:\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "stemmer=PorterStemmer()\r\n",
        "str=\"There are several types of stemming algorithms.\"\r\n",
        "str=word_tokenize(str)\r\n",
        "print(str)\r\n",
        "for word in str:\r\n",
        "  print(stemmer.stem(word))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['There', 'are', 'several', 'types', 'of', 'stemming', 'algorithms', '.']\n",
            "there\n",
            "are\n",
            "sever\n",
            "type\n",
            "of\n",
            "stem\n",
            "algorithm\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIrq5MEYqaJZ",
        "outputId": "db32e655-3096-4037-dd3c-8bf085bc8960"
      },
      "source": [
        "#Lemmatization using NLTK:\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "\r\n",
        "lemmatizer=WordNetLemmatizer()\r\n",
        "str=\"been had done languages cities mice\"\r\n",
        "str=word_tokenize(str)\r\n",
        "print(str)\r\n",
        "for word in str:\r\n",
        "  print(lemmatizer.lemmatize(word))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "['been', 'had', 'done', 'languages', 'cities', 'mice']\n",
            "been\n",
            "had\n",
            "done\n",
            "language\n",
            "city\n",
            "mouse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS3X-_0mrwnD",
        "outputId": "17517dba-33ee-4f7a-c223-c1c612d8e7e6"
      },
      "source": [
        "#Part-of-speech tagging using TextBlob\r\n",
        "import nltk\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "\r\n",
        "\r\n",
        "from textblob import TextBlob\r\n",
        "str=\"Parts of speech examples: an article, to write, interesting, easily, and, of\"\r\n",
        "res=TextBlob(str)\r\n",
        "print(\"str:\\n\",str)\r\n",
        "print(\"result :\\n\",res)\r\n",
        "print(\"result tags :\\n\",res.tags)\r\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "str:\n",
            " Parts of speech examples: an article, to write, interesting, easily, and, of\n",
            "result :\n",
            " Parts of speech examples: an article, to write, interesting, easily, and, of\n",
            "result tags :\n",
            " [('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), ('an', 'DT'), ('article', 'NN'), ('to', 'TO'), ('write', 'VB'), ('interesting', 'VBG'), ('easily', 'RB'), ('and', 'CC'), ('of', 'IN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-u6xZZKJeq5",
        "outputId": "f590ca10-87bf-4182-cc7e-24a88137e747"
      },
      "source": [
        "#Remove numbers\r\n",
        "s = '12abcd405'\r\n",
        "result = ''.join([i for i in s if not i.isdigit()])\r\n",
        "print(result)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "abcd\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}